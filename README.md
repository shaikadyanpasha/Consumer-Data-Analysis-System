This project implements a complete ETL and analytics pipeline for the Steam Games dataset. It begins with csv_to_sqlite.py, a Python script that uses pandas to load steam_games.csv and export it into a structured SQLite database (steam.db). From there, the processing shifts to C++: the sequential pipeline (Sequential.cpp) enforces single-core execution to provide a benchmarking baseline, loading rows via the SQLite C API, cleaning and structuring them into SteamGame objects, and applying custom parsing (regex/string ops) to prices, reviews, genres, and system requirements. It then computes analytics such as top-rated games, most frequent genres, and aggregated developer/publisher statistics, exporting results as auditable CSV files (e.g., formatted_debug.csv, system_requirements_summary.csv, top_5_games.csv, developer_stats.csv). Each stage is benchmarked with high-resolution timers, logging runtime vs. dataset size in size_vs_time_log.csv. The parallel pipeline (parallel.cpp) extends this system by distributing parsing and analytics across multiple CPU cores with std::thread, significantly improving throughput and scalability on datasets up to 40k rows. Parallel routines concurrently extract system requirements (CPU, GPU, RAM, storage, OS), compute analytics, and generate structured CSV reports (including publisher_stats.csv). Benchmarks demonstrate parallel speedups while preserving identical outputs. Together, the Python loader and C++ engines form a robust data-processing system: CSV → SQL → sequential or parallel C++ parsing, analytics, and benchmarking, providing both a baseline and an optimized multi-threaded implementation.

